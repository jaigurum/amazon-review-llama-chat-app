# model_analysis.ipynb

"""
This notebook is designed for evaluating and analyzing the performance of Llama models used in the Streamlit Chat application. The goal is to understand model behavior, fine-tune parameters, and benchmark the models.

## Sections Included:
- Loading the Model and Dataset
- Evaluation Metrics
- Model Benchmarking
- Analysis and Visualization
"""

import pandas as pd
import matplotlib.pyplot as plt
import ollama

# Load the model(s) to be analyzed
model_name = "llama3.1:8b"  # You can change to 'llama3.2:1b'
response = ollama.generate(model=model_name, prompt="Test prompt for evaluation")

# Placeholder for data collection and metrics
responses = []
metrics = []

# Iterate and gather responses for analysis
for _ in range(10):
    result = ollama.generate(model=model_name, prompt="Evaluation prompt")
    responses.append(result)
    # Metrics can include response time, accuracy of the response, etc.
    metrics.append({'response_time': result.get('response_time', 0)})

# DataFrame for metrics analysis
df_metrics = pd.DataFrame(metrics)

# Plotting metrics
plt.figure(figsize=(10, 5))
plt.plot(df_metrics['response_time'], label="Response Time")
plt.xlabel("Iteration")
plt.ylabel("Response Time (s)")
plt.title("Model Response Time Analysis")
plt.legend()
plt.show()
