import streamlit as st
import logging
import time
import ollama  # Official Ollama client

# Setup basic logging configuration
logging.basicConfig(level=logging.INFO)
def stream_chat(model, messages):
    try:
        # Combine all previous messages into a single prompt
        prompt = "\n".join([msg["content"] for msg in messages])
        logging.info(f"Sending prompt to the model: {prompt}")

        # Call the Ollama generate function to get the model's response
        response = ollama.generate(model=model, prompt=prompt)

        # Log the entire response to understand its structure
        logging.info(f"Full response: {response}")

        # Initialize an empty response text
        response_text = ""

        # Check if response is a dictionary
        if isinstance(response, dict):
            # Log all keys in response to see what we have
            for key, value in response.items():
                logging.info(f"Key: {key}, Value: {value}")

            # Assuming the actual text is under a specific key, like 'response'
            if 'response' in response:
                response_text = response['response']
            else:
                logging.warning("Expected key 'response' not found in the response.")
        else:
            logging.warning("Response is not a dictionary as expected.")

        # Log the final response text after processing
        if response_text:
            logging.info(f"Final response text: {response_text}")
        else:
            logging.warning("No response generated by the model.")

        # Display the response in Streamlit
        response_placeholder = st.empty()
        if response_text:
            response_placeholder.write(response_text)
        else:
            response_placeholder.write("No response generated by the model.")
        return response_text

    except Exception as e:
        logging.error(f"Error during streaming: {str(e)}")
        raise e

def main():
    st.title("Chat with LLMs Models")
    logging.info("App started")

    # Dropdown to select the model
    model = st.sidebar.selectbox("Choose a model", ["llama3.2:1b", "llama3.1:8b"])  # Updated model list with default option
    logging.info(f"Model selected: {model}")

    # Input box to capture the user's question
    if prompt := st.chat_input("Your question"):
        # Initialize session state to hold chat messages if not already initialized
        if 'messages' not in st.session_state:
            st.session_state.messages = []
        st.session_state.messages.append({"role": "user", "content": prompt})
        logging.info(f"User input: {prompt}")

        # Display previous messages
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.write(message["content"])

        # If the last message was not from the assistant, generate a new response
        if st.session_state.messages[-1]["role"] != "assistant":
            with st.chat_message("assistant"):
                start_time = time.time()
                logging.info("Generating response")

                with st.spinner("Writing..."):
                    try:
                        # Stream chat by sending all messages
                        response_message = stream_chat(model, st.session_state.messages)

                        # Measure the time taken to generate the response
                        duration = time.time() - start_time
                        response_message_with_duration = f"{response_message}\n\nDuration: {duration:.2f} seconds"
                        st.session_state.messages.append({"role": "assistant", "content": response_message_with_duration})
                        st.write(f"Duration: {duration:.2f} seconds")
                        logging.info(f"Response: {response_message}, Duration: {duration:.2f} s")

                    except Exception as e:
                        # Log the error and provide a detailed message
                        logging.error(f"Error during response generation: {str(e)}")
                        st.session_state.messages.append({"role": "assistant", "content": f"An error occurred: {str(e)}"})
                        st.error(f"An error occurred while generating the response: {str(e)}")
                        return  # Exit the function to prevent further execution

if __name__ == "__main__":
    # Initialize session state to hold chat messages if not already initialized
    if 'messages' not in st.session_state:
        st.session_state.messages = []
    main()
