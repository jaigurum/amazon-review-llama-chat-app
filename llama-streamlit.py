import streamlit as st
import logging
import time
import ollama  # Official Ollama client

logging.basicConfig(level=logging.INFO)

def stream_chat(model, prompt):
    try:
        # Call the Ollama generate function to get the model's response
        response = ollama.generate(model=model, prompt=prompt)
        
        # Initialize an empty response text
        response_text = ""

        # Log the full response to understand the structure
        logging.info(f"Full response: {response}")

        # Process the response to extract the 'response' field if present
        if isinstance(response, dict) and 'response' in response:
            response_text = response['response']
        else:
            logging.warning(f"Unexpected response structure: {response}")

        # Return response text
        return response_text

    except Exception as e:
        logging.error(f"Error during streaming: {str(e)}")
        return f"An error occurred: {str(e)}"

def main():
    st.title("Amazon Reviews Question Answering with Llama Model")
    logging.info("App started")

    # Dropdown to select the model
    model = st.sidebar.selectbox("Choose a model", ["llama3.2:1b", "llama3.1:8b"])
    logging.info(f"Model selected: {model}")

    # Sample questions to help users
    st.sidebar.subheader("Sample Questions:")
    sample_questions = [
        "What do customers think about [product name]?",
        "Are there any common complaints about [product category]?",
        "How do people describe the durability of [product name]?",
        "What features are customers happy about in [product]?"
    ]

    # Make sample questions clickable
    for question in sample_questions:
        if st.sidebar.button(question):
            st.session_state.prompt = question

    # Input box to capture the user's question
    if 'prompt' not in st.session_state:
        st.session_state.prompt = ""

    prompt = st.text_input("Ask a question about Amazon reviews:", value=st.session_state.prompt)
    if prompt:
        logging.info(f"User input: {prompt}")

        # Streamlit interface for displaying model response
        with st.spinner("Generating response..."):
            start_time = time.time()
            response_message = stream_chat(model, prompt)
            duration = time.time() - start_time

        # Display the response and duration
        if response_message:
            st.write(response_message)
            st.write(f"Response generated in {duration:.2f} seconds")
        else:
            st.error("No response generated by the model.")

if __name__ == "__main__":
    main()
